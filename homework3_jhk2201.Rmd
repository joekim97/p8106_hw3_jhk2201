---
title: "hw3_jhk2201"
author: "joseph Kim"
date: "10/18/2021"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(dplyr)
library(p8105.datasets)
data("instacart")
```

### Problem 1 (Description of Instacart Data)

###### How many aisles are there, and which aisles are the most items ordered from?

```{r}

max(instacart$aisle_id)

instacart1 <- instacart %>%
  group_by(aisle) %>% 
  summarise(order_amount=n()) %>% 
  arrange(desc(order_amount)) 
```

There are 134 total aisles listed in the dataframe. The aisles with the most items ordered from were fresh vegetables, fresh fruits, and packaged vegetables/fruits. 
 
----------

###### Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.

```{r}
instacart2 <- filter(instacart1, order_amount>10000) %>% 
arrange(desc(order_amount)) 

ggplot(instacart2, aes(x = aisle, y = order_amount)) + geom_bar(stat="Identity") +
coord_flip() + labs(title = "Number of Items Ordered for Aisle Greater than 10,000")
              
```

This plot shows the number of items ordered for each aisle that had greater than 10,000 items ordered. We see that the vegetables and fruits were by far the aisle in which most items were ordered from. 

----------

###### Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.

```{r results='asis'}
instacart %>%
      filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
      group_by(aisle, product_name) %>%
      summarize(amount_product = n()) %>%
      arrange(aisle, desc(amount_product)) %>%
      filter(min_rank(desc(amount_product)) <4) %>%
      knitr::kable(digits = 1, caption = "The Top Three Most Popular Items Sold in ")
```

The table produced shows the top three products from the aisles "baking ingredients", "dog food care", and "packaged vegetables fruits". The product name and the amount of each product sold is listed. 

----------

###### Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

```{r}

mean_hours = instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour_order = mean(order_hour_of_day)) %>%
  pivot_wider(names_from = order_dow, values_from = mean_hour_order) %>%
  rename(Sunday = "0", Monday = "1", Tuesday = "2", Wednesday = "3", Thursday = "4", Friday = "5", Saturday = "6") %>% 
  knitr::kable()

mean_hours
```

----------

### Problem 2 

##### Data cleaning the BRFSS dataset

```{r}
data("brfss_smart2010")

behavioral_health = brfss_smart2010 %>%
    janitor::clean_names() %>%
    filter(topic == "Overall Health") %>%
    filter(response %in% c("Excellent", "Very Good", "Good", "Fair", "Poor")) %>%
    mutate(response = as.factor(response)) %>%
    mutate(response = forcats::fct_relevel(response, c("Poor", "Fair", "Good", "Very good", "Excellent")))

behavioral_health
```

After data was cleaned, the resulting data frame had 8500 rows and 23 columns. 

----------

##### In 2002, which states were observed at 7 or more locations? What about in 2010?

```{r}
data("brfss_smart2010")

behavioral_health2002 = behavioral_health %>%
  filter(year == 2002) %>%
  group_by(locationabbr) %>%
  summarize(count = n_distinct(locationdesc)) %>%
  filter(count >= 7) %>%
  knitr::kable()

behavioral_health2002

behavioral_health2010 = behavioral_health %>%
  filter(year == 2010) %>%
  group_by(locationabbr) %>%
  summarize(count = n_distinct(locationdesc)) %>%
  filter(count >= 7) %>%
  knitr::kable()

behavioral_health2010
```

In 2002, there were six states that were observed at 7 or more locations 
(CT, FL, MA, NC, NJ, PA). In 2021, there were 14 states that fit the above criteria 
(CA, CO, FL, MA, MD, NC, NE, NJ, NY , OH, PA, SC, TX, WA).

----------

###### Make a “spaghetti” plot of this average value over time within a state.

```{r}
behave_excellent = behavioral_health %>%
  filter(response == "Excellent") %>%
  group_by(year, locationabbr) %>%
  summarize(avg_datavalue = mean(data_value, na.rm = TRUE)) 

ggplot(behave_excellent, aes(x = year, y = avg_datavalue, color = locationabbr)) + geom_line(aes(group = locationabbr), alpha =0.5) + labs(
    title = "Avergae Data Value Over Time By State",
    x = "Year",
    y = "Average Data Value",
    caption = "Data from brfss"
  )
```

The resulting spaghetti plot shows the change in average data value over time for each state. It appears very chaotic, and the data varies from state to state. 

----------

###### Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.

```{r}
plot_data = behavioral_health %>%
  filter(locationabbr == "NY") %>%
  filter(year %in% c("2006", "2008"))
  
ggplot(plot_data, aes(x=response, y=data_value)) + 
  geom_col() + facet_grid(~year) + labs(
    title = "Data_Value Counts for Response Types, By Year",
    x = "Response Types",
    y = "Data Values (%)",
    caption = "Data from brfss") 
```

The two paneled bar graphs produced shows the data value counts for response types separated by year (2006, 2008). More Good and Excellent response types are present in 2008 than in 2006. 

----------

### Problem 3 

###### Load, Clean, and Wrangle Data

```{r}
accelerometer_df=
  read_csv("/Users/josephkim/Desktop/p8106_hw3_jhk2201/accel_data.csv") %>%
  janitor::clean_names() %>%
    pivot_longer(
    activity_1:activity_1440,
    names_to = "minute_count", 
    values_to = "activity_count",
    names_prefix = "activity_") %>%
    mutate(
      week = as.integer(week), 
      day_id = as.integer(day_id), 
      minute_count = as.integer(minute_count), 
      weekend_weekday = case_when(
           day %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday") ~ "Weekday",
           day %in% c("Saturday", "Sunday") ~ "Weekend")) %>%
      relocate(weekend_weekday,.after = day)
```

The accelerometer data after tidying produced 50400 observations with 6 variables. Those variables include week, day_id, day, weekend_weekday, minute_count, and activity_count. We created the weekend_weekday was created with a case_when statement. 

----------

###### Using your tidied dataset, aggregate accross minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?

```{r}

agg.accel_df = accelerometer_df %>%
  group_by(week, day) %>%
  summarize(total_activity = sum(activity_count)) %>%
  knitr::kable(caption = "Total Activity Per Day of the Week")

agg.accel_df
```
 
The trends being aboved based on the table is that total daily activity is slower at the beginning of the week (Monday) and increases towards the weekend. This is not that consistent as you look through the weeks. 

----------

###### Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. 

```{r}

plot_data3 = accelerometer_df %>%
  group_by(day, minute_count)
  
ggplot(plot_data3, aes(x=minute_count, y=activity_count, color = day)) +
  geom_smooth(se = FALSE) + 
  labs(
   title = "Tracking Activity Over 24 Hours by Day",
    x = "Minutes during the day",
    y = "Total Activity Count") +
  scale_x_continuous(
    breaks = c(0, 120, 240, 360, 480, 600, 720, 840, 960, 1080, 1200, 1320, 1440), 
    labels = c("12 AM", "2 AM", "4 AM", "6 AM", "8 AM", "10 AM", "12 PM", "2 PM", "4 PM", "6 PM", "8 PM", "10 PM", "12 AM"))
```

Some apparent information from this plot is that there is an observed spike in activity during the mid day on Sundays. Additionally, we observe another spike on Friday from 7-9 pm, which could be when an individual may begin to enjoy their weekend after a week of work. 




